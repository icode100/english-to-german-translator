{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7P4Su-uiy2J"
      },
      "source": [
        "# English to German Language Converter using encoder and decoder model using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8k4eiQCjEsQ"
      },
      "source": [
        "---\n",
        "This model is trained on google colab with GPU suport.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lb4l5bsiy2P",
        "outputId": "038bb2de-17e6-4eaa-f6e1-2b15441bcabd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\",origin=url,cache_dir=\"datasets\",extract=True)\n",
        "text = (Path(path).with_name('spa-eng')/'spa.txt').read_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PywmQ7mziy2S"
      },
      "source": [
        "Each line contains an English sentence and the corresponding Spanish translation,\n",
        "separated by a tab. We’ll start by removing the Spanish characters “¡” and “¿”, which\n",
        "the TextVectorization layer doesn’t handle, then we will parse the sentence pairs\n",
        "and shuffle them. Finally, we will split them into two separate lists, one per language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lg4CzN3Yiy2U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\") # removing the special characters\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()] #splitting into 2 sep list\n",
        "np.random.shuffle(pairs) # shuffling the lists while maintaining the pair up order\n",
        "sentences_en, sentences_es = zip(*pairs) # naming them with different list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nLGlzGJiy2X",
        "outputId": "d173f37d-9b37-4f56-f97e-c8deab72e05c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The newcomers were quickly absorbed into the community. => Los recién llegados fueron rápidamente absorbidos a la comunidad.\n",
            "We have just a few more questions. => Tenemos sólo un par de preguntas más.\n",
            "I don't use it. => No lo uso.\n"
          ]
        }
      ],
      "source": [
        "# we demonstrate by pairing up the input and the target i.e., eng and spanish\n",
        "for i in list(zip(sentences_en[:3],sentences_es[:3])):\n",
        "    print(i[0],'=>',i[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o17uk6Ysiy2Y"
      },
      "outputs": [],
      "source": [
        "# creating two TextVectorization layers one for each language\n",
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "     vocab_size,output_sequence_length=max_length\n",
        ")\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "     vocab_size, output_sequence_length = max_length\n",
        ")\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])\n",
        "# For the Spanish text, we add “startofseq” and “endofseq” to each sentence when\n",
        "# adapting the TextVectorization layer: we will use these words as SOS and EOS\n",
        "# tokens. You could use any other words, as long as they are not actual Spanish\n",
        "# words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk2uxKnfiy2a",
        "outputId": "d5cf5bf3-a138-47e5-a3d1-b9f14c994df6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRYg3g-Qiy2b",
        "outputId": "1b12430d-41ec-465b-a5a1-81ab49c571eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XN_uiY_xiy2d"
      },
      "outputs": [],
      "source": [
        "# creating training and validation set.\n",
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "\n",
        "# The decoder’s inputs are the Spanish sentences plus an SOS token prefix\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "\n",
        "# The targets are the Spanish sentences plus an EOS suffix\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "du4cPtd6iy2g"
      },
      "outputs": [],
      "source": [
        "# building decoder-encoder model using functional API\n",
        "\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[],dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[],dtype=tf.string)\n",
        "#text vectorization of input layers\n",
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qGsZGB_fiy2h"
      },
      "outputs": [],
      "source": [
        "# creating the encoder and passing the embedded inputs\n",
        "\n",
        "encoder = tf.keras.layers.LSTM(512,return_state=True)\n",
        "# we set return_state = True so as to get a reference to the layers\n",
        "# final state\n",
        "encoder_outputs,*encoder_state = encoder(encoder_embeddings)\n",
        "# The layer returns these states separately, which is why we had to\n",
        "# write *encoder_state to group both states in a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1SbhCi6Liy2i"
      },
      "outputs": [],
      "source": [
        "# creating decoder\n",
        "# here we can use the double state (i.e., encoder_state) as the initial state of\n",
        "# the decoder\n",
        "decoder = tf.keras.layers.LSTM(512,return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings,initial_state=encoder_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LJVuMTMriy2j"
      },
      "outputs": [],
      "source": [
        "# generating the final output by as usual passing the decoder output\n",
        "# through a dense layer with softmax activation\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size,activation='softmax')\n",
        "Y_proba = output_layer(decoder_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0BCkft0iy2j",
        "outputId": "3b327ee4-85a1-46ea-e27e-6cabcc5fc102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 93s 26ms/step - loss: 2.9953 - accuracy: 0.4159 - val_loss: 2.2459 - val_accuracy: 0.5148\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 65s 21ms/step - loss: 1.9178 - accuracy: 0.5632 - val_loss: 1.7029 - val_accuracy: 0.6040\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 65s 21ms/step - loss: 1.4839 - accuracy: 0.6397 - val_loss: 1.4606 - val_accuracy: 0.6483\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 65s 21ms/step - loss: 1.2391 - accuracy: 0.6877 - val_loss: 1.3550 - val_accuracy: 0.6684\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 64s 20ms/step - loss: 1.0718 - accuracy: 0.7209 - val_loss: 1.3033 - val_accuracy: 0.6794\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 64s 20ms/step - loss: 0.9368 - accuracy: 0.7491 - val_loss: 1.2879 - val_accuracy: 0.6847\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 64s 20ms/step - loss: 0.8222 - accuracy: 0.7742 - val_loss: 1.2986 - val_accuracy: 0.6850\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 66s 21ms/step - loss: 0.7198 - accuracy: 0.7984 - val_loss: 1.3267 - val_accuracy: 0.6840\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 64s 21ms/step - loss: 0.6305 - accuracy: 0.8198 - val_loss: 1.3635 - val_accuracy: 0.6814\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 64s 21ms/step - loss: 0.5528 - accuracy: 0.8394 - val_loss: 1.4054 - val_accuracy: 0.6790\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7eecfc0c3a00>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compiling and fitting\n",
        "model = tf.keras.Model(inputs = [encoder_inputs,decoder_inputs], outputs = [Y_proba])\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='nadam',metrics=['accuracy'])\n",
        "model.fit((X_train,X_train_dec),Y_train,epochs=10,validation_data=((X_valid,X_valid_dec),Y_valid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WPtfXzaRLfQn"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/MyDrive/modelsML/eng_to_germ\", save_format='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-22 13:31:28.515687: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-02-22 13:31:28.616890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-22 13:31:28.617023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-22 13:31:28.619554: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-22 13:31:28.635922: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-02-22 13:31:28.637482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-22 13:31:30.881516: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-22 13:39:20.588181: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:20.624246: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:20.650465: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:20.720140: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:20.955063: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:20.986966: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:22.037523: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:22.801784: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:23.246224: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:23.299983: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:23.338582: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:23.365502: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:23.634942: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:23.656890: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:24.265946: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:24.289582: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:24.831434: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:24.853528: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:25.521656: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n",
            "2024-02-22 13:39:25.546376: W tensorflow/core/common_runtime/graph_constructor.cc:840] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "model = load_model(\"eng_to_germ\")\n",
        "\n",
        "# Use the model for prediction, evaluation, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 50\n",
        "import numpy as np\n",
        "def translate(sentence_en):\n",
        "    translation = ''\n",
        "    for word_idx in range(max_length):\n",
        "        X = np.array([sentence_en]) # encoder input\n",
        "        X_dec = np.array(['startofseq'+translation])# decoder input\n",
        "        y_proba = model.predict((X,X_dec))[0,word_idx] # last token's probas\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == 'endofseq':\n",
        "            break\n",
        "        translation += ' '+predicted_word\n",
        "    return translation.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'text_vec_layer_es' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy name is my name not of your name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(sentence_en)\u001b[0m\n\u001b[1;32m      8\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict((X,X_dec))[\u001b[38;5;241m0\u001b[39m,word_idx] \u001b[38;5;66;03m# last token's probas\u001b[39;00m\n\u001b[1;32m      9\u001b[0m predicted_word_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_proba)\n\u001b[0;32m---> 10\u001b[0m predicted_word \u001b[38;5;241m=\u001b[39m \u001b[43mtext_vec_layer_es\u001b[49m\u001b[38;5;241m.\u001b[39mget_vocabulary()[predicted_word_id]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_word \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendofseq\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_vec_layer_es' is not defined"
          ]
        }
      ],
      "source": [
        "translate(\"my name is my name not of your name\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
